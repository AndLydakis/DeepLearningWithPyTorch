{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Convolution in Action\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f35d1549a20>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "data_path = '../orig_data/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in connected_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3) #3 input channels, arbitrary output channels = 16\n",
    "# weight tensor = 16 output channels x 3 input channs x (3x3) kernel\n",
    "conv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-3c5bb8500af4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcifar2\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'conv' is not defined"
     ]
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.mean(0), cmap='gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAXHklEQVR4nO2dW4yc1ZWFv42xje228RXb+IINRiQOEQ5y0EiJSEZRCIMikbyg8BAxUjTOQ5ASKQ8TZR7CIxrlojyMIjkDChllcpFyQ0o0kwyKRJAihEHGXBqwMTa4ad8NGDB2297z0OVM4/RZp6nururhrE9qdfe/+6+z61Strqp/nb1PZCbGmPc/l/Q7AWNMb7DYjWkEi92YRrDYjWkEi92YRrDYjWmESydzckTcCnwfmAX8e2beq/5+7ty5uWDBgvETuVSnMm/evGLs/PnzKsdibPbs2XJMldOsWbOKsbNnzxZjb7/9thxTnXvJJeX/zbX7olBzpMZU59XiKqbmtsaZM2e6PreEekwAurWvR0ZGirFz5851Neabb77JO++8M+7kdi32iJgF/BvwaeAA8FhEPJiZz5bOWbBgAbfccsu4seXLl8vxPvShDxVjp06dKsaUYFevXi3HXLFiRTF2+eWXF2OHDx8uxnbt2iXHPHToUDE2f/78Ykzdl5oo58yZU4ypf7LqPND/gNQ/kUWLFhVjtReF/fv3F2PdivLYsWMyfvr06WJM3c+DBw8WY6+99lpXY/7ud78r5yJvUXMTsCcz92bmGeBnwO2TuD1jzDQyGbGvAV4Z8/uBzjFjzAxk2i/QRcS2iNgRETvU2x1jzPQyGbEPAevG/L62c+xdZOb2zNyamVvnzp07ieGMMZNhMmJ/DLg2IjZGxBzgC8CDU5OWMWaq6fpqfGaejYi7gf9m1Hq7PzOfUefMmzePzZs3jxt74YUX5HgvvvhiMaZsigMHDhRjmzZtkmNu2bKlGDty5EgxpiyV2rsbdVVduQ7qinHNdSjZoaCvxqsrzaDdA3WucjreeecdOaayNtXjovKpOQBq7lU+6mNtze47efLkuMeVFibls2fm74HfT+Y2jDG9wSvojGkEi92YRrDYjWkEi92YRrDYjWkEi92YRpiU9fZeueSSS4re6zXXXCPPVdVgGzZsKMZUxZfy7kH7q2vWlMsAlEf6+uuvyzFPnDjR1Ziqwkz58wBLliwpxpRXXivDVFVxAwMDxZjy2WtrBtR6jeHh4WJM+ey16j7FG2+8UYy99dZbXcWgvN5AlXv7ld2YRrDYjWkEi92YRrDYjWkEi92YRrDYjWmEnlpvIyMjxSZ7ixcvlueuWrWqGDt69GgxtnTp0mJMWTEAu3fvLsaUnaXsKmUT1s4dGvqb3iB/RVlSqoEjaLtG3U9ln8Fop9MSyrZTdtX1118vx1TxV155pRhTudbmT1m06vGcTOemkr2rmmr6ld2YRrDYjWkEi92YRrDYjWkEi92YRrDYjWmEnlpv586dK+5hVdtPS1lvquOosjdUFVktJ9WRVVkxe/bskWOuXLmyqzGVjVjraKu6rl522WXFWK0yS1X/KUtP5btz5045pspXVfep50ntualQc6TGrFUUlu6nqt7zK7sxjWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MIk7LeImIfcBI4B5zNzK3q70dGRjh8+PC4MWWLADz55JPF2IoVK4ox1YhR2XmgGw2qyixlm9Qq7UpVgVDPt5vbBG1XKStHVcuBrsBSc6SactbGVNZmtxtuLly4UI6pKuYmU9mm6KbqbSp89r/PzHKNqTFmRuC38cY0wmTFnsAfIuLxiNg2FQkZY6aHyb6N/3hmDkXEFcAfI+K5zHx47B90/glsA5g3b94khzPGdMukXtkzc6jz/TDwa+Cmcf5me2Zuzcytk9lZwxgzOboWe0QsiIiFF34GbgGenqrEjDFTy2Texq8Eft3ZS+1S4D8z87+mJCtjzJTTtdgzcy9ww3s8hzNnzowbq/mnqrzz1VdfLcZU+Wup3PYCqgR27dq1xZjybGudStXGhMovX79+fTFW+/ikfGL1uKjHBHRnVVXiqmJq40vQJbBqPYEqx609ZsqHV/On5r1Wllxa/+ASV2OMxW5MK1jsxjSCxW5MI1jsxjSCxW5MI/S8u2zJblCWAWj7Q5VLHjp0qBhTpbEAzz77bDGmbCWVq7LsAK677rpi7OTJk8WYsngmszGhsu1UOSXox0U93mqTytqYyoJU5aaqq2/N7lOlsyWrGbTdp84D/fwr4Vd2YxrBYjemESx2YxrBYjemESx2YxrBYjemEXpqvWVm0aZQ9gXoDfJUB1lVWaQq4mrs3r27GFu2bFkxVtuwb926dcWYstBUldTixYvlmMuXLy/Gjh4t9xKtzV+n/Hlc1OOprMCBgQE55oYNG4oxZXUpi7HW+VjZu+p+Xn755cWY6rDbLX5lN6YRLHZjGsFiN6YRLHZjGsFiN6YRLHZjGqGn1tv58+d5++23x43VqnxUNZNq+KduV9lyoC00VSWlLJ5aY01lOynrTeWqLDDofiPFWjXYrFmzirErrriiq9ut2X3KmlMWpLLBanapul3V5FJVMV511VVyzNI8KAvRr+zGNILFbkwjWOzGNILFbkwjWOzGNILFbkwjWOzGNELVZ4+I+4HPAocz8/rOsaXAz4ENwD7gjszUpuvoedJHVrz88svFmCrRVF6v8jkB5s2bV4ytWrWqGFOdP9VtgvZl1bnKf6517j127Fgxpja/HB4elrerPHqVk3o89+/fL8dUHYOVB63mtuazqzUXam2EWudRG7N0X37zm98Uz5nIK/uPgFsvOvYN4KHMvBZ4qPO7MWYGUxV7Zj4MHL/o8O3AA52fHwA+N8V5GWOmmG6Xy67MzAvv4Q4CxfcxEbEN2Aa6o4wxZnqZ9AW6HN2io7hNR2Zuz8ytmbm128/rxpjJ063YD0XEaoDO98NTl5IxZjroVuwPAnd1fr4L+O3UpGOMmS4mYr39FPgksDwiDgDfAu4FfhERXwL2A3dMZLCIKFouqsQQKJbGgradlPVWs6SOH7/4uuT/ocoar7766mKsdj+V7aQ2hVS2kpo70BaZst4WLFggb/fUqVPF2PPPP1+MqY97tcdMjak6vaoNI9Xcgn5M16xZU4wpe03lCuWSZmX7VsWemXcWQp+qnWuMmTl4BZ0xjWCxG9MIFrsxjWCxG9MIFrsxjdDzjR1LdsPp06fluWrjQmUdqQ0jld0CuhJK2UNqWfCmTZvkmMqqURaQqparde5Vc6vGPHjwoLxd1SVWnfvqq68WY7Wuq8qyUrbdkSNHirHayk9loXVbaVd7bpYq5mSlobxFY8z7BovdmEaw2I1pBIvdmEaw2I1pBIvdmEboqfUWEUVbSlXrXDi3G1TVm7LlQFd1KTtmyZIlxVit6k3ZYMrSU/aPsuVA56vmT+UKOt9Dhw4VY6oR6BtvvCHHVI1AVfXfZBpOKttO3Re1IWnNeivpQT2n/cpuTCNY7MY0gsVuTCNY7MY0gsVuTCNY7MY0gsVuTCP01GefPXt20QdVnmMtrsr6lC9b61Q6HZ1ea56tincbU/MD2ktXHryaH4CBgYFiTPnsg4ODxVhtVyG1jkGtjVAl1rX1GCon5cGrrsjr1q2TY5bWgMjOvPIWjTHvGyx2YxrBYjemESx2YxrBYjemESx2YxphIhs73g98Fjicmdd3jt0D/BNwwVf4Zmb+vnZbc+fOZePGjePGapsEKktFlScODQ0VY6+//rocU5XVKrtKnVfryKo2JlS3e/bs2WKs1l1W2U7KRlS2HGjLTz3eqtxZ2VWgO9OqMVWuNbuv286+atPMWin00qVLxz2uniMTeWX/EXDrOMe/l5lbOl9VoRtj+ktV7Jn5MFDeu9gY8/+CyXxmvzsidkXE/RGh388ZY/pOt2L/AXANsAUYBr5T+sOI2BYROyJih/o8aoyZXroSe2YeysxzmXke+CFwk/jb7Zm5NTO3qgtpxpjppSuxR8TqMb9+Hnh6atIxxkwXE7Hefgp8ElgeEQeAbwGfjIgtQAL7gC9PZLCBgQFuvvnmcWM1e0NZQMqmWLZsWTG2e/duOabaJLDbDQSVRQbaelPdXJXFU+vMqzqgHj58uBhT9iNoC03dbmnTQqjP36JFi4qxY8eOdRVTHWtBW2/qea2st1rn3pUrV457XHWlrYo9M+8c5/B9tfOMMTMLr6AzphEsdmMawWI3phEsdmMawWI3phEsdmMaoafdZefPn8+NN944bqzmBStPV63MU771Sy+9JMdUHVuVN/3KK68UYzXPVq0ZKJU1gt6hdHh4WI6p1gUoP7xWIqzKRlWpqlrDoNZbgC67VZ73iRMnijFVNguwevXqYkw9nuq5WSvlLT3n1Xl+ZTemESx2YxrBYjemESx2YxrBYjemESx2Yxqhp9bbnDlzWL9+/bixWgdUZSmoskdlK6myRtCdYNWYynLat2+fHFNtllgqa6zFajaOsuZU51m1aSbUrbkSam5rXVdVTsoi++AHP1iMPffcc3JMtUmlQm3CqKxdKJdn23ozxljsxrSCxW5MI1jsxjSCxW5MI1jsxjRCT623iChWt6kKM9BVXcrqUhVUtU6lylLp1gqs2WBqTFWBpuyzgYEBOabqcqoq+FQXWOjeQlPVfap7KuiurMpKVfl84hOfkGPu37+/GFNWoKp627RpkxyzpJfJbuxojHkfYLEb0wgWuzGNYLEb0wgWuzGNYLEb0wgT2dhxHfBjYCWjGzluz8zvR8RS4OfABkY3d7wjM8td+zqUGkfWrDdVfdXtvu+1LaTV7armheo8ZWWBtpaUlTUyMlKMKWsNtHWpLE9lZYG2gRYuXFiMKatw8eLFckzVkFJVQHYbA/joRz9ajKmmpspmrVWBXnnlleMeV8/LibyynwW+npmbgb8DvhIRm4FvAA9l5rXAQ53fjTEzlKrYM3M4M5/o/HwSGATWALcDD3T+7AHgc9OVpDFm8rynz+wRsQH4CPAosDIzLyzbOsjo23xjzAxlwmKPiAHgl8DXMvNdawBz9IPmuB82I2JbROyIiB1HjhyZVLLGmO6ZkNgjYjajQv9JZv6qc/hQRKzuxFcD4y7czsztmbk1M7euWLFiKnI2xnRBVewxekn1PmAwM787JvQgcFfn57uA3059esaYqWIiVW8fA74IPBUROzvHvgncC/wiIr4E7AfumJ4UjTFTQVXsmfkIUDJMP/VeBsvMallpCeU7Hj9+vBhTXTpVOSTAtddeW4zt2rWrGFNer9p4EHT32fnz5xdjyn+ulaKqub3sssuKMbXZJug1A2otgvLgP/CBD3Q9pnpc1DoOtQ4B9DwsWLCgGLviiiuKsVopb2ldijrPK+iMaQSL3ZhGsNiNaQSL3ZhGsNiNaQSL3ZhG6Gl32cmgNsFT1pHavLG28aAqtVQbAR44cKAYq21MqOwhhepaq+yzGsp2WrZsmTy32xJXZR+pEk7QNqN6XJStWbPeVAfZNWvWdHVereS7NEeyVFfeojHmfYPFbkwjWOzGNILFbkwjWOzGNILFbkwj9NR6y8yiRaS6mIKuXuu2A46yYkDbfatWrSrGlA22efNmOaaqtFP5qgqqvXv3yjFV1ZaqUqx1512+fHkxpqwlZZfWuvMuWrSoq5iqwjtxQjdNVvGrrrqqGNu4cWMxpjbxhHK+rnozxljsxrSCxW5MI1jsxjSCxW5MI1jsxjRCT623M2fOFO0jVQEE2j567rnnijG14WGtauuRRx7pKp+lS5cWYzW77+qrry7GVDNKZU3WKu1UVZeyyGoVesqCVDmp5o+Dg4NyTNXQU1mF3eYK8NZbbxVjQ0ND8twSGzZskPE9e/aMe9zWmzHGYjemFSx2YxrBYjemESx2YxrBYjemESayi+u6iPhTRDwbEc9ExFc7x++JiKGI2Nn5um360zXGdMtEfPazwNcz84mIWAg8HhF/7MS+l5nfnuhgIyMjDA8PjxurlfS99NJLxZgqezx69GgxpjpxgvYsS/cD9JqBWqdX1T113bp1XeVTQ82D2phwxYoV8nZLXjBob1+VoqryV4AXXnihGLvyyiuLMTXvNa9crddQaxH+8pe/FGO18uHSpqSqo+9EdnEdBoY7P5+MiEGg3B/XGDMjeU+f2SNiA/AR4NHOobsjYldE3B8Rei9iY0xfmbDYI2IA+CXwtcx8A/gBcA2whdFX/u8UztsWETsiYkdtUwZjzPQxIbFHxGxGhf6TzPwVQGYeysxzmXke+CFw03jnZub2zNyamVtra4yNMdPHRK7GB3AfMJiZ3x1zfPWYP/s88PTUp2eMmSomcjX+Y8AXgaciYmfn2DeBOyNiC5DAPuDL05KhMWZKmMjV+EeA8a7n//69Dnb69OlimeZrr70mz1XWktrMT9lytQ37VKmquv6gOrIqWwm0taQ2CVQbJdZsObWBpUJ12AVYvHhxMfbUU091NWZt/pRV2O3mjbWOtupcNQcl+wzgz3/+sxyztLGoKkn2CjpjGsFiN6YRLHZjGsFiN6YRLHZjGsFiN6YRetpddmRkpFhBpLp7gu44qmw7ZSvVNuxTFtratWuLMdVBtnY/b7jhhmJMdcpVFs/LL78sx1TzsH79+mKsthmnsgPVBpbKKjxz5owcU1lv6rmgqtNU5STofNW56jlU69xbes7bejPGWOzGtILFbkwjWOzGNILFbkwjWOzGNEJPrTeFavgH2qY4depUMbZ///5irFbtpaqkZs2aVYyppn+qiSXoTQJVg0dl1dSaFz766KPFmGqeqWwu0DaQ2sBSzVGtOlLZpWoe1H1RNiHA009318pB2XKXXqqlWXr+KXvWr+zGNILFbkwjWOzGNILFbkwjWOzGNILFbkwjWOzGNEJPffbz588XyyJrmyyquOpyqnzXWrmkGlP5oGrzxlqnUlUeW+uGW2LlypUy/pnPfKYY27t3bzFW2/RDlbhed911xdiHP/zhYmxwcFCOqTZhVB606vRaW4+hnn9qw1L13FTrG6Bc0qzWKPiV3ZhGsNiNaQSL3ZhGsNiNaQSL3ZhGsNiNaYSolVxO6WARR4CxNafLAd26s7c4H81MywdmXk79zueqzBy3FrqnYv+bwSN2ZObWviVwEc5HM9PygZmX00zLZyx+G29MI1jsxjRCv8W+vc/jX4zz0cy0fGDm5TTT8vkrff3MbozpHf1+ZTfG9Ii+iD0ibo2I5yNiT0R8ox85XJTPvoh4KiJ2RsSOPuVwf0QcjoinxxxbGhF/jIjdne9L+pzPPREx1JmnnRFxWw/zWRcRf4qIZyPimYj4aud4X+ZI5NO3OarR87fxETELeAH4NHAAeAy4MzOf7Wki785pH7A1M/vmj0bEzcCbwI8z8/rOsX8FjmfmvZ1/iksy85/7mM89wJuZ+e1e5HBRPquB1Zn5REQsBB4HPgf8I32YI5HPHfRpjmr045X9JmBPZu7NzDPAz4Db+5DHjCIzHwaOX3T4duCBzs8PMPpk6mc+fSMzhzPzic7PJ4FBYA19miORz4ylH2JfA4zt4HCA/k9SAn+IiMcjYlufcxnLysy8sPn3QUB3oegNd0fErs7b/J59rBhLRGwAPgI8ygyYo4vygRkwR+PhC3SjfDwzbwT+AfhK5y3sjCJHP2/12zr5AXANsAUYBr7T6wQiYgD4JfC1zHxXO5d+zNE4+fR9jkr0Q+xDwLoxv6/tHOsbmTnU+X4Y+DWjHzVmAoc6nw0vfEYs9zjqAZl5KDPPZeZ54If0eJ4iYjajwvpJZv6qc7hvczRePv2eI0U/xP4YcG1EbIyIOcAXgAf7kAcAEbGgc4GFiFgA3AJ0t3nX1PMgcFfn57uA3/YxlwtiusDn6eE8xegGevcBg5n53TGhvsxRKZ9+zlGVzOz5F3Abo1fkXwT+pR85jMnlauDJztcz/coH+Cmjb/tGGL2O8SVgGfAQsBv4H2Bpn/P5D+ApYBejIlvdw3w+zuhb9F3Azs7Xbf2aI5FP3+ao9uUVdMY0gi/QGdMIFrsxjWCxG9MIFrsxjWCxG9MIFrsxjWCxG9MIFrsxjfC/H4UVJ7hmXpYAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pad the input to maintain size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downsampling:\n",
    "* Average neighbors\n",
    "* Maximum of neighbors\n",
    "* Strided convolution where only every Nth pixel is calculated\n",
    "\n",
    "<img src=\"../resources/max_pooling.jpg\" alt=\"max_pooling\" height=\"200\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conv -> MaxPool\n",
    "<img src=\"../resources/conv_max.jpg\" alt=\"conv_max\" height=\"200\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),# 16 channel 32x32 image\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2), # 16 channel 16x16 image\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),  # 16 channel 8x8 image\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2), # 8 channel 8x8 image\n",
    "    ### <--- we need to convert 8c 8x8 to 512 dimensional 1D vector\n",
    "    nn.Linear(8*8*8, 32). # channel 8x8 image -> 1d vector -> 32 outputs\n",
    "    nn.Tanh(),\n",
    "    nn.Linearn(32, 2)\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Subclassing nn.Module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)# 16 channel 32x32 image\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2) # 16 channel 16x16 image\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)  # 16 channel 8x8 image\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2) # 8 channel 8x8 image\n",
    "        ### <--- we need to convert 8c 8x8 to 512 dimensional 1D vector\n",
    "        self.fc1 = nn.Linear(8*8*8, 32) # channel 8x8 image -> 1d vector -> 32 outputs\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "def forward(self, x):\n",
    "    out = self.pool1(self.act1(self.conv1(x)))\n",
    "    out = self.pool2(self.act2(self.conv2(out)))\n",
    "    out = out.view(-1, 8*8*8) # reshape for the fully connected\n",
    "    out = self.act3(self.fc1(out))\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The functional API"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0746, -0.0411]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "model(img.unsqueeze(0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # sum the losses over the epoch\n",
    "            # we need to transform them into a number with .item()\n",
    "            # to escape the gradients\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch%10==0:\n",
    "            print('{}, Epoch {}, training loss: {:.4f}'.format(\n",
    "                datetime.datetime.now(),\n",
    "                epoch,\n",
    "                loss_train / len(train_loader)\n",
    "            ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 13:39:20.754073, Epoch 1, training loss: 0.5687\n",
      "2021-03-07 13:39:29.432797, Epoch 10, training loss: 0.3355\n",
      "2021-03-07 13:39:39.414890, Epoch 20, training loss: 0.2978\n",
      "2021-03-07 13:39:49.020556, Epoch 30, training loss: 0.2717\n",
      "2021-03-07 13:39:59.049908, Epoch 40, training loss: 0.2506\n",
      "2021-03-07 13:40:08.673591, Epoch 50, training loss: 0.2321\n",
      "2021-03-07 13:40:18.270926, Epoch 60, training loss: 0.2134\n",
      "2021-03-07 13:40:28.089738, Epoch 70, training loss: 0.1958\n",
      "2021-03-07 13:40:37.540389, Epoch 80, training loss: 0.1801\n",
      "2021-03-07 13:40:47.187777, Epoch 90, training loss: 0.1655\n",
      "2021-03-07 13:40:56.882032, Epoch 100, training loss: 0.1532\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Batch 1/10000\n",
      "Accuracy train: 0.94\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Batch 1/2000\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_idx = 0\n",
    "            for imgs, labels in loader:\n",
    "                print('Batch {}/{}'.format(batch_idx+1, len(loader.dataset)))\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # index of highest output\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving And Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device) # move to training device\n",
    "            labels = labels.to(device=device) # move to training device\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # sum the losses over the epoch\n",
    "            # we need to transform them into a number with .item()\n",
    "            # to escape the gradients\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch%10==0:\n",
    "            print('{}, Epoch {}, training loss: {:.4f}'.format(\n",
    "                datetime.datetime.now(),\n",
    "                epoch,\n",
    "                loss_train / len(train_loader)\n",
    "            ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_idx = 0\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # index of highest output\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 13:42:35.366603, Epoch 1, training loss: 0.5849\n",
      "2021-03-07 13:42:38.285658, Epoch 10, training loss: 0.3386\n",
      "2021-03-07 13:42:41.477880, Epoch 20, training loss: 0.2959\n",
      "2021-03-07 13:42:44.773300, Epoch 30, training loss: 0.2660\n",
      "2021-03-07 13:42:47.901390, Epoch 40, training loss: 0.2479\n",
      "2021-03-07 13:42:50.998146, Epoch 50, training loss: 0.2321\n",
      "2021-03-07 13:42:54.032933, Epoch 60, training loss: 0.2134\n",
      "2021-03-07 13:42:57.004925, Epoch 70, training loss: 0.1977\n",
      "2021-03-07 13:43:00.048431, Epoch 80, training loss: 0.1818\n",
      "2021-03-07 13:43:03.143679, Epoch 90, training loss: 0.1664\n",
      "2021-03-07 13:43:06.220574, Epoch 100, training loss: 0.1523\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=100,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading from device\n",
    "\n",
    "There is a slight complication when loading network weights: PyTorch will attempt\n",
    "to load the weight to the same device it was saved fromthat is, weights on the GPU\n",
    "will be restored to the GPU. As we dont know whether we want the same device, we have two options:\n",
    "we could move the network to the CPU before saving it, or move it back after restoring.\n",
    "It is a bit more concise to instruct PyTorch to override the device information when loading weights.\n",
    "This is done by passing the map_location keyword argument to torch.load"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_net = Net().to(device=device)\n",
    "loaded_net.load_state_dict(torch.load(data_path+ 'birds_vs_airplanes.pt', map_location=device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adding memory capacity: Width\n",
    "\n",
    "Increase the number of output channels of the first conv layer and increase the subsequent\n",
    "layers accordingly to increase the number of neurons per layer.\n",
    "\n",
    "The input vector to the fully connected layers is now longer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 13:49:08.198537, Epoch 1, training loss: 0.5813\n",
      "2021-03-07 13:49:11.555307, Epoch 10, training loss: 0.3130\n",
      "2021-03-07 13:49:15.633943, Epoch 20, training loss: 0.2680\n",
      "2021-03-07 13:49:19.245098, Epoch 30, training loss: 0.2341\n",
      "2021-03-07 13:49:22.875753, Epoch 40, training loss: 0.2088\n",
      "2021-03-07 13:49:26.462401, Epoch 50, training loss: 0.1834\n",
      "2021-03-07 13:49:30.445728, Epoch 60, training loss: 0.1611\n",
      "2021-03-07 13:49:34.146667, Epoch 70, training loss: 0.1421\n",
      "2021-03-07 13:49:37.811810, Epoch 80, training loss: 0.1204\n",
      "2021-03-07 13:49:41.672031, Epoch 90, training loss: 0.1028\n",
      "2021-03-07 13:49:45.650649, Epoch 100, training loss: 0.0861\n",
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avoid hardcoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1=n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, self.n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(self.n_chans1, self.n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(self.n_chans1 // 2 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, self.n_chans1 // 2 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 13:55:29.952036, Epoch 1, training loss: 0.5487\n",
      "2021-03-07 13:55:33.308334, Epoch 10, training loss: 0.3168\n",
      "2021-03-07 13:55:37.196466, Epoch 20, training loss: 0.2704\n",
      "2021-03-07 13:55:41.084795, Epoch 30, training loss: 0.2387\n",
      "2021-03-07 13:55:45.007794, Epoch 40, training loss: 0.2104\n",
      "2021-03-07 13:55:48.908261, Epoch 50, training loss: 0.1874\n",
      "2021-03-07 13:55:52.792369, Epoch 60, training loss: 0.1613\n",
      "2021-03-07 13:55:56.631365, Epoch 70, training loss: 0.1410\n",
      "2021-03-07 13:56:00.340169, Epoch 80, training loss: 0.1204\n",
      "2021-03-07 13:56:04.082919, Epoch 90, training loss: 0.1008\n",
      "2021-03-07 13:56:07.989878, Epoch 100, training loss: 0.0848\n",
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.89\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_acc_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-e3dbcecd550a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0mall_acc_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"width\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalidate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'all_acc_dict' is not defined"
     ]
    }
   ],
   "source": [
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "width_acc = validate(model, train_loader, val_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Assisting Convergence and Generalization: Regularization\n",
    "\n",
    "Keep weigts small by regularizing loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def training_l2_reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 14:04:24.024999 Epoch 1, Training loss 0.5967912924517492\n",
      "2021-03-07 14:04:28.280230 Epoch 10, Training loss 0.35454992636753496\n",
      "2021-03-07 14:04:33.070121 Epoch 20, Training loss 0.31809191272896564\n",
      "2021-03-07 14:04:37.894564 Epoch 30, Training loss 0.3005345251150192\n",
      "2021-03-07 14:04:42.764859 Epoch 40, Training loss 0.2836620528606852\n",
      "2021-03-07 14:04:47.456098 Epoch 50, Training loss 0.2679176411241483\n",
      "2021-03-07 14:04:52.143992 Epoch 60, Training loss 0.254248198857353\n",
      "2021-03-07 14:04:56.938559 Epoch 70, Training loss 0.24001503175800773\n",
      "2021-03-07 14:05:01.822721 Epoch 80, Training loss 0.23103937478202163\n",
      "2021-03-07 14:05:06.626075 Epoch 90, Training loss 0.22021847735544678\n",
      "2021-03-07 14:05:11.434333 Epoch 100, Training loss 0.21507341324523754\n",
      "Accuracy train: 0.91\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_l2_reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "l2_reg_acc = validate(model, train_loader, val_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Avoid relying on specific features: Dropout\n",
    "\n",
    "Randomly drop a percentage of features\n",
    "\n",
    "Dropout is normally active during training, while during the evaluation of a trained model\n",
    "in production, dropout is bypassed or, equivalently, assigned a probability  equal  to  zero.\n",
    "This  is  controlled  through  the  train  property  of  the  Dropoutmodule.\n",
    "Recall that PyTorch lets us switch between the two modalities by calling ```model.train()```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 14:06:06.639198 Epoch 1, Training loss 0.6012234627061589\n",
      "2021-03-07 14:06:11.629613 Epoch 10, Training loss 0.4152898824518653\n",
      "2021-03-07 14:06:17.174882 Epoch 20, Training loss 0.37919276297851734\n",
      "2021-03-07 14:06:22.868144 Epoch 30, Training loss 0.3592615479686458\n",
      "2021-03-07 14:06:28.423148 Epoch 40, Training loss 0.345863278409478\n",
      "2021-03-07 14:06:33.676768 Epoch 50, Training loss 0.3318553808957908\n",
      "2021-03-07 14:06:39.278375 Epoch 60, Training loss 0.3172346600301706\n",
      "2021-03-07 14:06:44.656007 Epoch 70, Training loss 0.31052531112151543\n",
      "2021-03-07 14:06:49.996902 Epoch 80, Training loss 0.2987490235620244\n",
      "2021-03-07 14:06:55.442558 Epoch 90, Training loss 0.2857463319020666\n",
      "2021-03-07 14:07:00.871634 Epoch 100, Training loss 0.282200960976303\n",
      "Accuracy train: 0.90\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetDropout().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_l2_reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "dropout_acc = validate(model, train_loader, val_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Keep Activations in check: Batch Normalization\n",
    "\n",
    "Rescale  the  inputs  to  the  activations of the network so that minibatches have a\n",
    "certain desirable distribution. Recalling  the  mechanics  of  learning  and  the\n",
    "role  of  nonlinear  activation  functions,  this helps avoid the inputs to activation\n",
    "functions being too far into the saturated portion of the function, thereby killing\n",
    "gradients and slowing training.\n",
    "\n",
    "Batch normalization needs to behave differently during training and inference. In fact,\n",
    "at inference time, we want to avoid having the output for a specific input depend on\n",
    "the statistics of the other inputs were presenting to the model.As such, we need\n",
    "a way to still normalize, but this time fixing the normalization parameters once and for all."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 14:10:34.541267, Epoch 1, training loss: 0.4611\n",
      "2021-03-07 14:10:38.827288, Epoch 10, training loss: 0.2689\n",
      "2021-03-07 14:10:43.255299, Epoch 20, training loss: 0.2149\n",
      "2021-03-07 14:10:47.722341, Epoch 30, training loss: 0.1752\n",
      "2021-03-07 14:10:52.166492, Epoch 40, training loss: 0.1352\n",
      "2021-03-07 14:10:56.826976, Epoch 50, training loss: 0.1038\n",
      "2021-03-07 14:11:01.404597, Epoch 60, training loss: 0.0791\n",
      "2021-03-07 14:11:06.027523, Epoch 70, training loss: 0.0604\n",
      "2021-03-07 14:11:10.799376, Epoch 80, training loss: 0.0479\n",
      "2021-03-07 14:11:15.555004, Epoch 90, training loss: 0.0783\n",
      "2021-03-07 14:11:20.457271, Epoch 100, training loss: 0.0235\n",
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "batch_norm_acc = validate(model, train_loader, val_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Deeper Models\n",
    "\n",
    "#### Skip Connections\n",
    "The addition of the input to the outputof a block of layers.\n",
    "\n",
    "A skip connection, or a sequence of skip connections in a deep network, creates a direct\n",
    "path from the deeper parameters to the loss. This makes their contribution to the gradient\n",
    "of the loss more direct, as partial derivatives of the loss with respect to those parameters\n",
    "have a chance not to be multiplied by a long chain of other operations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans//2, n_channs//2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4*4*self.n_chans1//2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 14:21:44.423236, Epoch 1, training loss: 0.5420\n",
      "2021-03-07 14:21:48.207998, Epoch 10, training loss: 0.3139\n",
      "2021-03-07 14:21:51.956420, Epoch 20, training loss: 0.2708\n",
      "2021-03-07 14:21:55.866836, Epoch 30, training loss: 0.2446\n",
      "2021-03-07 14:22:00.088820, Epoch 40, training loss: 0.2184\n",
      "2021-03-07 14:22:03.945831, Epoch 50, training loss: 0.1929\n",
      "2021-03-07 14:22:07.799790, Epoch 60, training loss: 0.1673\n",
      "2021-03-07 14:22:11.612050, Epoch 70, training loss: 0.1483\n",
      "2021-03-07 14:22:15.200494, Epoch 80, training loss: 0.1289\n",
      "2021-03-07 14:22:18.786798, Epoch 90, training loss: 0.1089\n",
      "2021-03-07 14:22:22.547138, Epoch 100, training loss: 0.0915\n",
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "acc_depth = validate(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding the skip connection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1//2, n_chans1//2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4*4*self.n_chans1//2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-07 14:24:42.420106, Epoch 1, training loss: 0.6272\n",
      "2021-03-07 14:24:46.216318, Epoch 10, training loss: 0.3230\n",
      "2021-03-07 14:24:50.307285, Epoch 20, training loss: 0.2893\n",
      "2021-03-07 14:24:54.475468, Epoch 30, training loss: 0.2550\n",
      "2021-03-07 14:24:58.646141, Epoch 40, training loss: 0.2211\n",
      "2021-03-07 14:25:02.803910, Epoch 50, training loss: 0.1975\n",
      "2021-03-07 14:25:06.893813, Epoch 60, training loss: 0.1741\n",
      "2021-03-07 14:25:10.944850, Epoch 70, training loss: 0.1536\n",
      "2021-03-07 14:25:15.037150, Epoch 80, training loss: 0.1261\n",
      "2021-03-07 14:25:19.114513, Epoch 90, training loss: 0.1074\n",
      "2021-03-07 14:25:23.205997, Epoch 100, training loss: 0.0891\n",
      "Accuracy train: 0.94\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetRes().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "acc_depth = validate(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Block Composition\n",
    "\n",
    "<img src=\"../resources/block_compose.jpg\" alt=\"block_compose\" height=\"200\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans=32):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans\n",
    "        self.conv2d = nn.Conv2d(self.n_chans, self.n_chans, kernel_size=3, padding=1, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=self.n_chans)\n",
    "        # random initialization\n",
    "        torch.nn.init.kaimin_normal_(self.conv2d.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant(self.batchnorm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batchnorm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.batchnorm(self.conv2d(x)))\n",
    "        return out + x\n",
    "\n",
    "class ResNetDeep(nn.Module):\n",
    "    def __init__(self, n_chans=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans\n",
    "        self.conv1 = nn.Conv2d(3, n_chans, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans)])\n",
    "        )\n",
    "        self.fc1 = nn.Linear(8*8*n_chans, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = view(-1, 8*8*n_chans)\n",
    "        out = fc1(out)\n",
    "        out = fc2(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ResNetDeep().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "acc_resnet = validate(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}